# -*- coding: utf-8 -*-
"""llama-finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ySKn4aQmsNGVtwh5NRTEZQfaGhjStsFI
"""

!pip install peft transformers datasets accelerate --quiet

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q cudf-cu12 --extra-index-url=https://pypi.nvidia.com

!pip install -q huggingface_hub

from huggingface_hub import login

# Paste your token between the quotes â†“
login(token="<HF_TOKEN>")

import cudf
import torch
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, EarlyStoppingCallback
from datasets import load_dataset, Dataset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

device

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-3B")
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-3B",
    torch_dtype=torch.bfloat16,         # bf16 is great on A100
    device_map="auto",
    attn_implementation="sdpa",
    low_cpu_mem_usage=True,
)

tokenizer.pad_token_id = tokenizer.eos_token_id

stream = load_dataset("teknium/OpenHermes-2.5", split="train")

df = cudf.from_pandas(stream.to_pandas())

df = df[["conversations"]].head(200_000)

df_cpu = df.to_pandas()

df_cpu["human"] = df_cpu["conversations"].apply(lambda x: x[0]["value"])
df_cpu["gpt"] = df_cpu["conversations"].apply(lambda x: x[1]["value"])

df = cudf.from_pandas(df_cpu)

df.drop(columns=["conversations"], inplace=True)

def build_text(row):
    return (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        "You are a helpful assistant.\n"
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n"
        f"{row['human']}\n"
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
        f"{row['gpt']}"
    )

def tokenize_and_mask(row):
    text = build_text(row)
    enc = tokenizer(text, truncation=True, max_length=1024, padding="max_length", return_tensors=None)
    # Find the assistant segment start to mask loss properly
    assistant_tag = "<|start_header_id|>assistant<|end_header_id|>"
    split_idx = text.index(assistant_tag) + len(assistant_tag)
    pre_ids = tokenizer(text[:split_idx], add_special_tokens=False)["input_ids"]
    labels = enc["input_ids"][:]
    # mask everything before assistant content
    labels[:len(pre_ids)] = [-100] * len(pre_ids)
    enc["labels"] = labels
    return enc

df_cpu = df.to_pandas()
hf_ds = Dataset.from_pandas(df_cpu, preserve_index=False)

tokenized_ds = hf_ds.map(tokenize_and_mask)

def is_valid(example):
    return len(example["input_ids"]) > 10 and len(example["labels"]) == len(example["input_ids"])

tokenized_ds = tokenized_ds.filter(is_valid)

tokenized_ds

tokenized_ds = tokenized_ds.remove_columns(["human", "gpt"])

from sklearn.model_selection import train_test_split

ds = tokenized_ds.train_test_split(test_size=0.05, shuffle=True, seed=42)
train_ds = ds["train"]
eval_ds = ds["test"]

train_ds

# ðŸ”´ REQUIRED when using gradient checkpointing on decoder LMs:
model.config.use_cache = False

# ðŸ”´ Do this BEFORE adding LoRA
model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
model.gradient_checkpointing_enable()
# Some versions still need this explicitly:
if hasattr(model, "enable_input_require_grads"):
    model.enable_input_require_grads()

model.config.use_cache = False

# LoRA on attention + MLP linear layers
lora_cfg = LoraConfig(
    task_type="CAUSAL_LM",
    r=16, lora_alpha=32, lora_dropout=0.05, bias="none",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)
peft_model = get_peft_model(model, lora_cfg)

# Collator for causal LM (builds labels from input_ids)
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
    pad_to_multiple_of=8
)

training_args = TrainingArguments(
    output_dir="./llama_finetune_lora",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    num_train_epochs=1,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    weight_decay=0.01,
    logging_steps=200,

    eval_strategy="steps",
    eval_steps=200,
    save_strategy="steps",
    save_steps=1000,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    bf16=True,
    fp16=False,
    gradient_checkpointing=True,
    torch_compile=False,
    report_to="none",
    seed=42
)

trainer = Trainer(
    model=peft_model,
    args=training_args,
    data_collator=None,
    train_dataset=train_ds,       # HF Dataset with input_ids/attention_mask
    eval_dataset=eval_ds,
    tokenizer=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]
)

peft_model.print_trainable_parameters()

try:
    trainer.train()
    tokenizer.save_pretrained("./llama_finetune_lora")
except Exception as e:
    print(f"Training error: {e}")

model = model.merge_and_unload() # To download Entire model weights and stuff and not just the LoRA

peft_model.push_to_hub("kunjcr2/llama3-3b-lora-openhermes", commit_message="Add LoRA adapters")
tokenizer.push_to_hub("kunjcr2/llama3-3b-lora-openhermes")

